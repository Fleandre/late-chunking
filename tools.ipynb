{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调整benchmark.json以使其合法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "OUTPUT_DIR = \"results\"\n",
    "\n",
    "def load_existing_results():\n",
    "    evaluated_key = dict()\n",
    "    # 读取已有的结果\n",
    "    with open(f\"{OUTPUT_DIR}/benchmark.json\", \"r\") as f:\n",
    "        content = json.load(f)\n",
    "    correct = []\n",
    "    for idx, eval_res in enumerate(content):\n",
    "        if eval_res is None:\n",
    "            continue\n",
    "        if eval_res[\"task_name\"] in [\n",
    "            \"SciFactChunked\",\n",
    "            \"NFCorpusChunked\",\n",
    "            \"FiQA2018Chunked\",\n",
    "            \"LEMBWikimQARetrievalChunked\",\n",
    "            \"SCIDOCSChunked\",\n",
    "            \"CmedqaRetrievalChunked\",\n",
    "            \"CovidRetrievallChunked\",\n",
    "            \"DuRetrievalChunked\",\n",
    "            \"T2RetrievalChunked\",\n",
    "        ] and eval_res[\"chunking_strategy\"] == \"semantic_langchain\" and \"bce\" not in eval_res[\"model_name\"]:\n",
    "            continue\n",
    "        eval_setting = {\n",
    "            \"task_name\": eval_res[\"task_name\"],\n",
    "            \"model_name\": eval_res[\"model_name\"],\n",
    "            \"chunking_strategy\": eval_res[\"chunking_strategy\"],\n",
    "            \"chunk_size\": eval_res[\"chunk_size\"],\n",
    "        }\n",
    "        key = json.dumps(eval_setting, sort_keys=True)\n",
    "        if key in evaluated_key:\n",
    "            print(f\"key: {key}\")\n",
    "            print(f\"found@{idx}: {eval_res}\")\n",
    "            print(f\"duplicate@{evaluated_key[key][1]}: {evaluated_key[key][0]}\")\n",
    "            print(\"\\n\\n\")\n",
    "            continue\n",
    "        correct.append(eval_res)\n",
    "        evaluated_key[key] = (eval_res, idx)\n",
    "    with open(f\"{OUTPUT_DIR}/benchmark.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(correct, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "load_existing_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计各数据集的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zyh/miniconda3/envs/chunking/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Map: 100%|██████████| 1406/1406 [00:00<00:00, 26776.45 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArguAnaChunked: NumOfQueries: 1406, NumOfCorpus:8674, total: 10080, average_corpus_length: 1054.4003919760203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4681/4681 [00:00<00:00, 38471.56 examples/s]\n",
      "Map: 100%|██████████| 5673/5673 [00:00<00:00, 42874.67 examples/s]\n",
      "Filter: 100%|██████████| 467/467 [00:00<00:00, 98816.46 examples/s]\n",
      "Map: 100%|██████████| 140085/140085 [00:03<00:00, 43588.90 examples/s]\n",
      "Filter: 100%|██████████| 123142/123142 [00:00<00:00, 512098.62 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClimateFEVERChunked: NumOfQueries: 1535, NumOfCorpus:5416593, total: 5418128, average_corpus_length: 563.2428945649045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 43515/43515 [00:01<00:00, 41679.02 examples/s]\n",
      "Filter: 100%|██████████| 467/467 [00:00<00:00, 170000.00 examples/s]\n",
      "Map: 100%|██████████| 8079/8079 [00:00<00:00, 39267.97 examples/s]\n",
      "Filter: 100%|██████████| 123142/123142 [00:00<00:00, 594758.00 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBPediaChunked: NumOfQueries: 400, NumOfCorpus:4635922, total: 4636322, average_corpus_length: 334.4551515318851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7937/7937 [00:00<00:00, 39013.28 examples/s]\n",
      "Filter: 100%|██████████| 123142/123142 [00:00<00:00, 567393.59 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEVERChunked: NumOfQueries: 6666, NumOfCorpus:5416568, total: 5423234, average_corpus_length: 563.2350253887702\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import concurrent.futures\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "import numpy as np\n",
    "from chunked_pooling.chunked_eval_tasks import *\n",
    "from chunked_pooling.wrappers import load_model\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "task_name_to_cls = get_eval_tasks()\n",
    "model_name = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "model, has_instructions = load_model(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "def task(task_name):\n",
    "    task_cls = task_name_to_cls[task_name]\n",
    "    chunking_args = {\n",
    "        \"chunk_size\": 1024,\n",
    "        \"n_sentences\": 5,\n",
    "        \"chunking_strategy\": \"fixed_text\",\n",
    "        \"model_has_instructions\": has_instructions,\n",
    "        \"embedding_model_name\": model_name,\n",
    "    }\n",
    "\n",
    "    task = task_cls(\n",
    "        tokenizer=tokenizer,\n",
    "        prune_size=None,\n",
    "        truncate_max_length=False,\n",
    "        **chunking_args,\n",
    "    )\n",
    "\n",
    "    task.load_data()\n",
    "    sub_set = \"test\" if \"test\" in task.queries else \"dev\"\n",
    "    queries = task.queries[sub_set]\n",
    "    corpus = task.corpus[sub_set]\n",
    "\n",
    "    doc_len = np.array([len(str(doc)) for doc in corpus.values()])\n",
    "    mean_len = doc_len.mean()\n",
    "    \n",
    "\n",
    "    num_queries = len(queries)\n",
    "    num_corpus = len(corpus)\n",
    "    print(f\"{task_name}: NumOfQueries: {num_queries}, NumOfCorpus:{num_corpus}, total: {num_queries+num_corpus}, average_corpus_length: {mean_len}\")\n",
    "    return {task_name: {\"queries\": num_queries, \"corpus\": num_corpus, \"average_corpus_length\": mean_len}}\n",
    "\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = list(executor.map(task, task_name_to_cls.keys()))\n",
    "    sample_count = {k: v for d in results for k, v in d.items()}\n",
    "\n",
    "    with open(\"sample_count.json\", \"w\") as f:\n",
    "        json.dump(sample_count, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open(\"master.log\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "dedup = set()\n",
    "for line in lines:\n",
    "    if line in dedup:\n",
    "        print(\"Duplicate: \" + line)\n",
    "    dedup.add(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalQasEn2Zh\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalLawEn2Zh\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalBooksEn2Zh\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalFinanceEn2Zh\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalPaperEn2Zh\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalWikiEn2Zh\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalLawZh2En\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalBooksZh2En\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalFinanceZh2En\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalPaperZh2En\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalWikiZh2En\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalQasEn2Zh-qrels\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalLawEn2Zh-qrels\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalBooksEn2Zh-qrels\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalFinanceEn2Zh-qrels\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalPaperEn2Zh-qrels\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalWikiEn2Zh-qrels\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalLawZh2En-qrels\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalBooksZh2En-qrels\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalFinanceZh2En-qrels\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalPaperZh2En-qrels\")\n",
    "load_dataset(\"maidalun1020/CrosslingualRetrievalWikiZh2En-qrels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计各个数据集Corpus文本的长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import concurrent.futures\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "\n",
    "import numpy as np\n",
    "from chunked_pooling.chunked_eval_tasks import *\n",
    "from chunked_pooling.wrappers import load_model\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "task_name_to_cls = get_eval_tasks()\n",
    "model_name = \"jinaai/jina-embeddings-v2-small-en\"\n",
    "model, has_instructions = load_model(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def task(task_name):\n",
    "    task_cls = task_name_to_cls[task_name]\n",
    "    chunking_args = {\n",
    "        \"chunk_size\": 1024,\n",
    "        \"n_sentences\": 5,\n",
    "        \"chunking_strategy\": \"fixed_text\",\n",
    "        \"model_has_instructions\": has_instructions,\n",
    "        \"embedding_model_name\": model_name,\n",
    "    }\n",
    "\n",
    "    task = task_cls(\n",
    "        tokenizer=tokenizer,\n",
    "        prune_size=None,\n",
    "        truncate_max_length=False,\n",
    "        **chunking_args,\n",
    "    )\n",
    "\n",
    "    task.load_data()\n",
    "    sub_set = \"test\" if \"test\" in task.queries else \"dev\"\n",
    "    corpus = task.corpus[sub_set]\n",
    "\n",
    "    doc_len = {task_name: np.array([len(str(doc)) for doc in corpus.values()])}\n",
    "    \n",
    "    return doc_len\n",
    "\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    doc_len_list = list(executor.map(task, task_name_to_cls.keys()))\n",
    "    np.save(\"doc_len.npy\", doc_len_list)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chunking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
